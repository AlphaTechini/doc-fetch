package fetcher

import (
	"fmt"
	"regexp"
	"sort"
	"strings"
)

// ContentGroup represents a logically grouped section of documentation
type ContentGroup struct {
	Title       string
	Description string
	Pages       []PageContent
	Order       int // For sorting
}

// PageContent represents content from a single page
type PageContent struct {
	URL     string
	Title   string
	Content string
	Level   int // Heading level (h1=1, h2=2, etc.)
}

// ContentGrouper organizes fetched content intelligently
type ContentGrouper struct {
	groups        map[string]*ContentGroup
	pageExtractor *PageTypeExtractor
}

// NewContentGrouper creates a new content grouper
func NewContentGrouper() *ContentGrouper {
	return &ContentGrouper{
		groups:        make(map[string]*ContentGroup),
		pageExtractor: NewPageTypeExtractor(),
	}
}

// AddPage adds a page to appropriate group
func (cg *ContentGrouper) AddPage(url, title, content string) {
	// Extract page type and section
	pageType := cg.pageExtractor.ExtractType(url, title)
	section := cg.extractSection(url, title)
	
	// Create or get group
	groupKey := fmt.Sprintf("%s_%s", pageType.Category, section)
	group, exists := cg.groups[groupKey]
	if !exists {
		group = &ContentGroup{
			Title:       cg.formatGroupTitle(pageType.Category, section),
			Description: pageType.Description,
			Pages:       make([]PageContent, 0),
			Order:       cg.getOrderForCategory(pageType.Category),
		}
		cg.groups[groupKey] = group
	}
	
	// Add page to group
	group.Pages = append(group.Pages, PageContent{
		URL:     url,
		Title:   title,
		Content: cg.cleanContent(content),
		Level:   cg.detectHeadingLevel(title),
	})
}

// GenerateMarkdown produces well-organized markdown output
func (cg *ContentGrouper) GenerateMarkdown() string {
	var sb strings.Builder
	
	// Write header
	sb.WriteString("# Documentation\n\n")
	sb.WriteString("Generated by DocFetch - Intelligent documentation fetcher\n\n")
	sb.WriteString("---\n\n")
	
	// Write table of contents
	sb.WriteString("## Table of Contents\n\n")
	cg.writeTableOfContents(&sb)
	
	// Write grouped content
	sortedGroups := cg.getSortedGroups()
	for i, group := range sortedGroups {
		if i > 0 {
			sb.WriteString("\n---\n\n")
		}
		cg.writeGroup(&sb, group)
	}
	
	return sb.String()
}

// writeTableOfContents generates a structured TOC
func (cg *ContentGrouper) writeTableOfContents(sb *strings.Builder) {
	sortedGroups := cg.getSortedGroups()
	
	for _, group := range sortedGroups {
		// Group heading
		groupID := cg.slugify(group.Title)
		fmt.Fprintf(sb, "- [%s](#%s)\n", group.Title, groupID)
		
		// Pages in group
		for _, page := range group.Pages {
			pageID := cg.slugify(page.Title)
			fmt.Fprintf(sb, "  - [%s](#%s)\n", page.Title, pageID)
		}
	}
	sb.WriteString("\n")
}

// writeGroup writes a single content group
func (cg *ContentGrouper) writeGroup(sb *strings.Builder, group *ContentGroup) {
	// Group header
	fmt.Fprintf(sb, "## %s\n\n", group.Title)
	
	if group.Description != "" {
		fmt.Fprintf(sb, "%s\n\n", group.Description)
	}
	
	// Sort pages within group
	cg.sortPages(group.Pages)
	
	// Write each page
	for _, page := range group.Pages {
		// Page heading based on detected level
		prefix := strings.Repeat("#", page.Level+1)
		fmt.Fprintf(sb, "%s %s\n\n", prefix, page.Title)
		
		// Add source link
		fmt.Fprintf(sb, "*Source: [%s](%s)*\n\n", page.URL, page.URL)
		
		// Content
		fmt.Fprintf(sb, "%s\n\n", page.Content)
	}
}

// Helper methods

func (cg *ContentGrouper) extractSection(url, title string) string {
	// Try to extract section from URL
	parts := strings.Split(strings.TrimSuffix(url, "/"), "/")
	if len(parts) > 0 {
		lastPart := parts[len(parts)-1]
		if lastPart != "" && !strings.Contains(lastPart, ".") {
			return cg.humanize(lastPart)
		}
	}
	
	// Fallback to first part of title
	fields := strings.Fields(title)
	if len(fields) > 0 {
		return fields[0]
	}
	
	return "general"
}

func (cg *ContentGrouper) cleanContent(content string) string {
	// Remove excessive whitespace
	content = regexp.MustCompile(`\n{3,}`).ReplaceAllString(content, "\n\n")
	content = regexp.MustCompile(`  +`).ReplaceAllString(content, " ")
	
	// Remove navigation elements that slipped through
	content = regexp.MustCompile(`(?m)^(Table of Contents|Contents|On this page).*?\n{2,}`).ReplaceAllString(content, "")
	
	// Remove breadcrumb trails
	content = regexp.MustCompile(`(?m)^Home\s*â€º.*?\n{2,}`).ReplaceAllString(content, "")
	
	return strings.TrimSpace(content)
}

func (cg *ContentGrouper) detectHeadingLevel(title string) int {
	// Count words - shorter titles tend to be higher level
	words := len(strings.Fields(title))
	
	if words <= 3 {
		return 1 // H2 (H1 is document title)
	} else if words <= 6 {
		return 2 // H3
	}
	return 3 // H4
}

func (cg *ContentGrouper) formatGroupTitle(category, section string) string {
	category = cg.humanize(category)
	section = cg.humanize(section)
	
	if section == "general" {
		return category
	}
	return fmt.Sprintf("%s - %s", category, section)
}

func (cg *ContentGrouper) getOrderForCategory(category string) int {
	// Define preferred order
	order := map[string]int{
		"getting-started": 1,
		"installation":    2,
		"quickstart":      3,
		"tutorial":        4,
		"guide":           5,
		"api":             6,
		"reference":       7,
		"example":         8,
		"faq":             9,
		"troubleshoot":    10,
	}
	
	if order, exists := order[category]; exists {
		return order
	}
	return 100 // Unknown categories go last
}

func (cg *ContentGrouper) getSortedGroups() []*ContentGroup {
	groups := make([]*ContentGroup, 0, len(cg.groups))
	for _, group := range cg.groups {
		groups = append(groups, group)
	}
	
	sort.Slice(groups, func(i, j int) bool {
		return groups[i].Order < groups[j].Order
	})
	
	return groups
}

func (cg *ContentGrouper) sortPages(pages []PageContent) {
	sort.Slice(pages, func(i, j int) bool {
		return pages[i].Title < pages[j].Title
	})
}

func (cg *ContentGrouper) slugify(text string) string {
	text = strings.ToLower(text)
	text = regexp.MustCompile(`[^a-z0-9]+`).ReplaceAllString(text, "-")
	text = strings.Trim(text, "-")
	return text
}

func (cg *ContentGrouper) humanize(text string) string {
	// Replace hyphens/underscores with spaces
	text = regexp.MustCompile(`[-_]`).ReplaceAllString(text, " ")
	
	// Title case
	words := strings.Fields(text)
	for i, word := range words {
		if len(word) > 0 {
			words[i] = strings.ToUpper(string(word[0])) + strings.ToLower(word[1:])
		}
	}
	
	return strings.Join(words, " ")
}

// PageTypeExtractor identifies the type of documentation page
type PageTypeExtractor struct {
	patterns     map[string]*regexp.Regexp
	descriptions map[string]string
}

// PageTypeInfo holds extracted page information
type PageTypeInfo struct {
	Category    string
	Description string
}

// NewPageTypeExtractor creates a new page type extractor
func NewPageTypeExtractor() *PageTypeExtractor {
	return &PageTypeExtractor{
		patterns: map[string]*regexp.Regexp{
			"getting-started": regexp.MustCompile(`(?i)(getting[-_]?started|intro|overview)`),
			"installation":    regexp.MustCompile(`(?i)(install|setup|configure|build)`),
			"quickstart":      regexp.MustCompile(`(?i)(quick[-_]?start|hello|5[-_]?min)`),
			"tutorial":        regexp.MustCompile(`(?i)(tutorial|walkthrough|guide|learn)`),
			"api":             regexp.MustCompile(`(?i)(api|reference|spec|endpoint)`),
			"example":         regexp.MustCompile(`(?i)(example|sample|demo|cookbook)`),
			"faq":             regexp.MustCompile(`(?i)(faq|question|answer)`),
			"troubleshoot":    regexp.MustCompile(`(?i)(trouble|debug|issue|problem|error)`),
		},
		descriptions: map[string]string{
			"getting-started": "Introduction and basic concepts",
			"installation":    "Installation and setup instructions",
			"quickstart":      "Quick start guides and tutorials",
			"tutorial":        "Step-by-step tutorials",
			"api":             "API reference documentation",
			"example":         "Examples and code samples",
			"faq":             "Frequently asked questions",
			"troubleshoot":    "Troubleshooting and debugging",
		},
	}
}

// ExtractType determines the page type from URL and title
func (pe *PageTypeExtractor) ExtractType(url, title string) PageTypeInfo {
	text := url + " " + title
	
	for category, pattern := range pe.patterns {
		if pattern.MatchString(text) {
			return PageTypeInfo{
				Category:    category,
				Description: pe.descriptions[category],
			}
		}
	}
	
	// Default category
	return PageTypeInfo{
		Category:    "general",
		Description: "General documentation",
	}
}
